{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aedbc2d6",
   "metadata": {},
   "source": [
    "# An√°lisis Inmobiliario\n",
    "\n",
    "1. Introducci√≥n\n",
    "2. Exploraci√≥n del Dataset\n",
    "3. Limpieza de datos\n",
    "4. An√°lisis Exploratorio de Datos (EDA)\n",
    "5. Correlaciones \n",
    "6. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4d54a",
   "metadata": {},
   "source": [
    "# 1 Introducci√≥n:\n",
    "# Insights del Mercado de Vivienda de Melbourne en 2017: Inmersi√≥n en los datos inmobiliarios\n",
    "\n",
    "![Insights Inmobiliarios](casas.jpg)\n",
    "\n",
    "Varios colegas de mi universidad han emigrado hacia Australia, en donde han encontrado oportunidades para desarrollarse como profesionales,  algunos de ellos me han contado de su inter√©s por vivir una temporada larga all√≠, por lo que me entr√≥ la curiosidad. ¬øCu√°nto puede costar una casa en dicho pa√≠s? As√≠ que decid√≠ escoger este data set por lo mismo, aunque es del a√±o 2017, recopila informaci√≥n sobre propiedades vendidas, cada fila representa el historial o trazabilidad de una propiedad que fue vendida y cada columna nos da el detalle de dicha propiedad, precio, ubicaci√≥n entre otros. \n",
    "\n",
    "La informaci√≥n proviene de un dataset p√∫blico que enocntr√© en Kaggle [https://www.kaggle.com/dansbecker/melbourne-housing-snapshot](https://www.kaggle.com/dansbecker/melbourne-housing-snapshot) La informaci√≥n fue recopilada y organizada a partir de anuncios extraidos del sitio web:[https://www.domain.com.au/](https://www.domain.com.au/).\n",
    "\n",
    "Respecto a las variables que se encuentran en el archivo te muestro lo que significa cada columna de estos datos:\n",
    "\n",
    "* **Rooms:** El n√∫mero de cuartos que tiene el lugar, que nos ayuda a entender el tama√±o de la propiedad.\n",
    "* **Price:** El precio en d√≥lares australianos al que se vendi√≥, decisivo en el an√°lisis del costo de las propiedades en ese momento.\n",
    "* **Method:** Una abreviatura de c√≥mo se vendi√≥ (por ejemplo, 'S' si fue una venta normal, 'SP' si se vendi√≥ antes de la subasta). Esto nos da una idea de las estrategias de venta.\n",
    "* **Type:** Si era una casa ('h'), un apartamento ('u'), etc. Conocer el tipo nos permite comparar diferentes opciones de vivienda.\n",
    "* **SellerG:** El nombre del agente o la agencia que vendi√≥ la propiedad.\n",
    "* **Date:** El d√≠a en que se cerr√≥ la venta.\n",
    "* **Distance:** Qu√© tan lejos est√° del centro de Melbourne. Esta distancia puede influir en el precio y la conveniencia de la ubicaci√≥n.\n",
    "* **Regionname:** La zona de Melbourne donde est√° la propiedad (por ejemplo, el oeste, el norte), que nos ayuda a analizar el mercado por √°reas.\n",
    "* **Propertycount:** Cu√°ntas propiedades hay en ese mismo barrio.\n",
    "* **Bedroom2:** El n√∫mero de habitaciones seg√∫n otra fuente de datos que encontr√©.\n",
    "* **Bathroom:** Cu√°ntos ba√±os tiene.\n",
    "* **Car:** Cu√°ntos puestos para estacionar carros hay.\n",
    "* **Landsize:** El tama√±o del terreno en metros cuadrados.\n",
    "* **BuildingArea:** Cu√°ntos metros cuadrados construidos tiene la propiedad.\n",
    "* **CouncilArea:** El municipio al que pertenece.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfdfc4c",
   "metadata": {},
   "source": [
    "# 2. Exploraci√≥n del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librer√≠a pandas para visualizar en modo de tabla en este notebook nuestros datos\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ruta: Creamos una variable para almacenar la ubicaci√≥n del archivo CSV, que nos ayuda acortar el codigo.\n",
    "\n",
    "data_ruta = \"Data/melb_data.csv\"\n",
    "\n",
    "# Leemos el archivo CSV utilizando la funci√≥n read_csv de pandas y lo cargamos en un DataFrame llamado: data.\n",
    "data = pd.read_csv(data_ruta)\n",
    "\n",
    "# Utilizamos el m√©todo head(5) para mostrar las primeras 5 filas del DataFrame 'data', lo que nos da\n",
    "# una vista r√°pida del contenido del dataset.\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f674f9",
   "metadata": {},
   "source": [
    "# 3. Limpieza de datos\n",
    "\n",
    "### Detecci√≥n de Valores Faltantes: Buscando los \"huecos\" en la informaci√≥n\n",
    "\n",
    "As√≠ como para cher es fundamental que un oufit no est√© incompleto, debemos revisar nuestro DataSet, ¬øTiene valores faltantes?\n",
    "\n",
    "Asi que debemos ir a nuestro Dtaframe 'data' y revisar si hay informaci√≥n que falta, esto es muy importante porque si hay una columna con falta de informaci√≥n la confiabilidad de nuestros an√°lisis podr√≠a disminuir dr√°sticamente.\n",
    "\n",
    "Dado de esto podemos usar la funci√≥n de pandas **'.isnull()'** ¬øPara que sirve? Este m√©todo que crea una nueva tabla del mismo tama√±o, pero reemplaza los datos originales por True y False.\n",
    "\n",
    "True:Cuando hay un valor faltante y se observa como Nan (not a number).\n",
    "False: Cuando hay informaci√≥n.\n",
    "\n",
    "As√≠ que gracias a este m√©todo podemos tener una data con verdaderos y falsos lo que nos ayuda a identificar facilemte los \"huecos\" en nuestra informaci√≥n.\n",
    "\n",
    "Extraido de [documentaci√≥n oficial de pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed39d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos este m√©todo y visualizamos los primeros 5 resultados\n",
    "\n",
    "is_null_result = data.isnull()\n",
    "print(is_null_result.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb9918",
   "metadata": {},
   "source": [
    "### Detecci√≥n de Valores no Faltantes: Buscando los \"huecos\" en la informaci√≥n con .notnul()\n",
    "\n",
    "\n",
    "Tenemos tambi√©n otro m√©todo, pero en este caso devuelve valor que **no son nulos** es denominado **.notnull()** \n",
    "\n",
    "\n",
    "False:Cuando hay un valor faltante y se observa como Nan (not a number).\n",
    "True: Cuando hay informaci√≥n.\n",
    "\n",
    "En resumen .notnull es el inverso de .isnull\n",
    "Extra√≠do de: [documentaci√≥n oficial de pandas para `.notnull()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.notnull.html) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizamos este m√©todo para visualizar los primeros resultados\n",
    "\n",
    "is_notnull_result = data.notnull()\n",
    "print(is_notnull_result.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77139b6f",
   "metadata": {},
   "source": [
    "### Conteo de los \"huecos\" por cada columna de la tabla\n",
    "\n",
    "Ahora debemos averiguar que partes de nuestra informaci√≥n est√°n incompletas para tomar una decisi√≥ns al respecto, recordando el m√©todo de **.isnull()** cada \"Verdadero\" (que marcaba un hueco) es como un 1 y cada \"Falso\" (donde s√≠ hab√≠a informaci√≥n) es como un 0.\n",
    "\n",
    "Si sumamos todos esos 1s y 0s por cada columna de nuestra tabla de \"Verdadero\" y \"Falso\" (la que obtuvimos con **.isnull()** el resultado ser√° **el n√∫mero total de \"Verdaderos\"**, lo que es igual a la cantidad de valores faltantes en esa columna.\n",
    "\n",
    "Entonces para contar cu√°ntos valores nulos hay en cada columna de nuestro data set debemos sumar los resultados de \".isnull()\" por cada columna de la siguiente manera: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_nulls = is_null_result.sum()\n",
    "print(cant_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181fce95",
   "metadata": {},
   "source": [
    "### Conteo de Elementos No Nulos por Columna\n",
    "\n",
    "Para ver cu√°ntos elementos no nulos tenemos en cada columna de nuestro DataFrame (`data`), podemos utilizar el m√©todo `.count()`. Este m√©todo itera a trav√©s de cada columna y devuelve el n√∫mero de valores que no son considerados nulos (`NaN`).\n",
    "\n",
    "El c√≥digo para realizar este conteo es el siguiente:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e2434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_null_column_count(data):\n",
    "    result = data.count()\n",
    "    return(result)\n",
    "\n",
    "resultado = not_null_column_count(data)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb8599",
   "metadata": {},
   "source": [
    "### Eliminaci√≥n de Valores Faltantes (\"Drop\")\n",
    "\n",
    "Al tener identificados los valores faltantes de nuestro Dataframe debemos darles una soluci√≥n, tenemos diferentes caminos, uno de ellos es la eliminaci√≥n que consiste en borrar filas columnas en donde est√©n estos valores faltantes.\n",
    "\n",
    "√âste es un m√©todo sencillo de aplicar pero podemos tener perdida de informaci√≥n valiosa y posibles sesgos si hay una relaci√≥n con otras variables. \n",
    "\n",
    "Implentar√≠a √©ste m√©todo cuando pueda observar que la ausencia de los datos en un conjunto de datos es aleatoria, en este caso considero que debo eliminar los datos en donde los valores faltantes sean de mas del 40%; Sin embargo detallo a continuaci√≥n las formas de usar pandas para realizar esta operaci√≥n usando **dropna()**\n",
    "\n",
    "    \n",
    "#### Opciones de Eliminaci√≥n con **dropna()** en pandas\n",
    "\n",
    "* **Eliminar columnas con *alg√∫n* valor faltante:**\n",
    "    ```python\n",
    "    data_sin_nulos_columnas = data.dropna(axis=1)\n",
    "    ```\n",
    "\n",
    "* **Eliminar filas con *todos* los valores faltantes:**\n",
    "    ```python\n",
    "    data_sin_filas_todo_nulo = data.dropna(how='all')\n",
    "    ```\n",
    "\n",
    "* **Eliminar columnas con *todos* los valores faltantes:**\n",
    "    ```python\n",
    "    data_sin_columnas_todo_nulo = data.dropna(axis=1, how='all')\n",
    "    ```\n",
    "\n",
    "* **Eliminar filas con menos de un umbral de valores no nulos:** (Ejemplo: eliminar filas con menos de 3 valores no nulos)\n",
    "    ```python\n",
    "    data_sin_filas_pocos_validos = data.dropna(thresh=3)\n",
    "    ```\n",
    "\n",
    "* **Eliminar columnas con menos de un umbral de valores no nulos:** (Ejemplo: eliminar columnas con menos de 5 valores no nulos)\n",
    "    ```python\n",
    "    data_sin_columnas_pocos_validos = data.dropna(axis=1, thresh=5)\n",
    "    ```\n",
    "\n",
    "Documentaci√≥n de pandas: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Descartando Columnas con \"Demasiados\" Huecos (Umbral del 40%)\n",
    "\n",
    "#En lugar de borrar sin pensar cualquier columna con un solo dato faltante, queremos ser un poco m√°s inteligentes. \n",
    "#La idea ahora es **eliminar solo aquellas columnas que est√©n \"demasiado vac√≠as\"**.\n",
    "\n",
    "#Para esto, vamos a usar un **l√≠mite**, o como lo llamamos aqu√≠, un **\"umbral\"**. \n",
    "#En este caso, nuestro umbral es del 40% (o 0.4). Esto significa que **si una columna tiene el 40% o m√°s \n",
    "#de sus datos como \"huecos\" (nulos), entonces decidiremos que esa columna no nos aporta mucha informaci√≥n √∫til y la vamos a eliminar.**\n",
    "\n",
    "#As√≠, conservaremos las columnas que, aunque tengan algunos datos faltantes, todav√≠a tienen una buena cantidad de informaci√≥n completa.\n",
    "\n",
    "#Para hacer esto, vamos a crear una funci√≥n llamada `drop_columns_umbral`. Esta funci√≥n va a recibir dos cosas:\n",
    "\n",
    "#* `data`: Que es nuestra tabla de datos (el DataFrame).\n",
    "#* `umbral`: Que es ese l√≠mite que definimos (en este caso, 0.4).\n",
    "\n",
    "#La funci√≥n deber√° revisar cada columna de nuestra tabla `data` y ver qu√© tan \"llena\" est√°. Si una columna est√° \"vac√≠a\" en un 40% o m√°s de sus filas, la funci√≥n la eliminar√° y nos devolver√° una tabla `limpia` con solo las columnas que s√≠ tienen suficiente informaci√≥n.\n",
    "\n",
    "\n",
    "def drop_columns_umbral(data, umbral):\n",
    "    # Aqu√≠ vamos a escribir el c√≥digo para revisar cada columna\n",
    "    # y eliminar las que tengan un porcentaje de nulos mayor o igual al 'umbral'.\n",
    "    result = None\n",
    "    return(result)\n",
    "\n",
    "umbral = 0.4\n",
    "# 'data' es la tabla de datos que ya cargamos.\n",
    "resultado = drop_columns_umbral(data, umbral)\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_umbral(data, umbral=0.4, verbose=True):\n",
    "    # 1. Calculamos el porcentaje de valores nulos por columna\n",
    "    percent_null = data.isnull().mean()\n",
    "    \n",
    "    # 2. Columnas que s√≠ cumplen con el umbral (es decir, que tienen menos nulos del permitido)\n",
    "    columnas_conservadas = percent_null[percent_null < umbral].index\n",
    "    \n",
    "    # 3. Columnas que vamos a eliminar\n",
    "    columnas_eliminadas = percent_null[percent_null >= umbral].index\n",
    "    \n",
    "    # 4. Mostrar las columnas eliminadas si verbose est√° en True\n",
    "    if verbose:\n",
    "        print(f\"Columnas eliminadas (m√°s del {umbral*100}% nulos):\")\n",
    "        print(columnas_eliminadas.tolist())\n",
    "\n",
    "    # 5. Crear dos DataFrames: uno limpio y otro solo con las columnas eliminadas\n",
    "    data_limpia = data[columnas_conservadas].copy()\n",
    "    data_columnas_eliminadas = data[columnas_eliminadas].copy()\n",
    "\n",
    "    return data_limpia, data_columnas_eliminadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_limpia, columnas_eliminadas = drop_columns_umbral(data, umbral=0.4, verbose=True)\n",
    "\n",
    "# Si m√°s adelante quieres visualizarlas:\n",
    "columnas_eliminadas.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b83d1",
   "metadata": {},
   "source": [
    "### Llenando los \"Huecos\": Imputaci√≥n de Valores Faltantes\n",
    "\n",
    "Es com√∫n tener informaci√≥n faltante en un conjunto de datos, por lo que debemos decidir que hacer con esa informaci√≥n si no se elimina se deben llenar con infromaci√≥n esto se denomina imputaci√≥n, hay diferentes formas de hacerlo c√≥mo:\n",
    "\n",
    "\n",
    "**1. Rellenado Simple:**\n",
    "\n",
    "Se basa en estimar el valor faltante con la informaci√≥n que ya tenemos en la columna o fila.\n",
    "\n",
    "**2. Rellenado M√∫ltiple:**\n",
    "\n",
    "Se basa en poner varios posibles valares en el lugar del dato faltante, se debe analizar y combinar los resultados para tener una idea mas clara de la incertumbre de estos datos faltantes.\n",
    "\n",
    "**3. Rellenar con el Promedio (Media):**\n",
    "\n",
    "Una forma de llenar este hueco cuando los valores son s√∫mericos es hallar la media que es el promedio de datos de esa columna, es algo bastante sencillo, PERO si tenemos outliers puede ser un gran problema ya que los datos pueden variar enormemente. \n",
    "\n",
    "\n",
    "**4. Rellenar con un Dato Existente (Hot Deck):**\n",
    "\n",
    "Es seleccionar un valor al azar de otra parte de la columna, puede ser antes o despu√©s del \"hueco\" o uno con condiciones similares. \n",
    "\n",
    "**5. Rellenar usando Regresiones:**\n",
    "\n",
    "Esta t√©nica se basa en usar otras columnas de la tabla para intentar predecir el valor faltante, se debe construir un modelo para adivinar el dato.\n",
    "\n",
    "**C√≥mo pandas nos ayuda a rellenar los huecos:**\n",
    "\n",
    "La librer√≠a pandas de Python tiene herramientas muy √∫tiles para la imputaci√≥n, la principal es: **fillna()** y se puede usar de diferentes formas:\n",
    "\n",
    "* Usar un valor fijo: Podemos decidir llenar todos los huecos con un n√∫mero espec√≠fico (como 0) o un texto.\n",
    "\n",
    "* Usar el valor anterior o siguiente: Ideal para datos que cambian con el tiempo (series de tiempo). Podemos copiar el √∫ltimo valor v√°lido hacia adelante **ffill** o el siguiente valor v√°lido hacia atr√°s **bfill** para llenar los huecos.\n",
    "\n",
    "* Usar estad√≠sticas: Podemos usar la media **mean()**, la moda **mode()** o la mediana **median()** de la columna para rellenar los valores faltantes. Por ejemplo: **data.fillna(data.mean())**\n",
    "\n",
    "En nuestros datos, vimos que las columnas con datos faltantes son 'Car', 'BuildingArea', 'YearBuilt' y 'CouncilArea'. \n",
    "\n",
    "Ahora vamos a intentar rellenar esos huecos usando algunas de estas ideas. Pero antes, vamos a echar un vistazo a los valores que ya tienen esas columnas para entender mejor qu√© m√©todo de relleno podr√≠a ser el m√°s adecuado. Para esto, usaremos herramientas que ya aprendimos para ver c√≥mo se distribuyen los datos.\n",
    "\n",
    "Despu√©s de esto, usaremos visualizaciones del tipo histogramas y distribuciones, para decidir como llenar los valores faltantes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2700a",
   "metadata": {},
   "source": [
    "### Echando un Vistazo a los \"Huecos\" que tenemos\n",
    "\n",
    "\n",
    "\n",
    "* Car: Le faltan 62 datos.\n",
    "* BuildingArea: Tenemos 6450 datos faltantes\n",
    "* YearBuilt: 5375 datos faltantes\n",
    "* CouncilArea: Le faltan 1369 datos.\n",
    "\n",
    "Vamos a mirar variable por variable para entender mejor que informaci√≥n contiene cada columna, para esto aplicamos estad√≠stica descriptiva y probabilidad.\n",
    "\n",
    "Es muy importante tener en cuenta si tenemos valores at√≠picos, outliers mediante las distribuciones a graficar como la normal y de esta manera poder elegir el mejor m√©todo de imputaci√≥n para cada columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def count_plotter(data, label, labelsize=15, palette=\"pastel\"):    \n",
    "    sns.set(rc={\"figure.figsize\": (10, 8), \n",
    "                \"xtick.labelsize\": labelsize})\n",
    "    sns.set_style(\"white\")    \n",
    "    data_count = sns.countplot(x=data, palette=palette)\n",
    "    data_count.set_title('Histograma de ' + label + '\\n', fontsize=18)\n",
    "\n",
    "    \n",
    "#def distribution_plotter(data, label, bins=None):    \n",
    "def distribution_plotter(data, label, bin_width=150, color=\"#FF6F61\"):    \n",
    "    sns.set(rc={\"figure.figsize\": (10, 8)})\n",
    "    sns.set_style(\"white\")    \n",
    "    #dist = sns.distplot(data, bins= bins, hist_kws={'alpha':0.2}, kde_kws={'linewidth':5})\n",
    "    dist = sns.histplot(data, \n",
    "                        stat = 'density', kde = True, \n",
    "                        line_kws = {'linewidth':6}, \n",
    "                        binwidth = bin_width,\n",
    "                        color=color,  # color del histograma\n",
    "                        kde_kws={'linewidth': 3, 'color': '#34568B'})  # color del KDE)        \n",
    "    dist.set_title('Distribucion de ' + label + '\\n', fontsize=18)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d68f67",
   "metadata": {},
   "source": [
    "# Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observamos la forma de los valores de este campo \n",
    "count_plotter(data.Car, \"Car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d0516",
   "metadata": {},
   "source": [
    "Podemos usar tambi√©n este m√©todo: **value_counts** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(data.Car)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f7323",
   "metadata": {},
   "source": [
    "### Rellenando los \"Huecos\" en la Cantidad de Cocheras ('Car')\n",
    "\n",
    "En la columna \"car\" podemos observar que la mayor√≠a de propiedades tienen 1 o 2 parqueaderos que son los valores mas comunes o probables.\n",
    "\n",
    "Vamos a probar dos formas diferentes de rellenar los 62 valores faltantes que encontramos en esta columna:\n",
    "\n",
    "**Opci√≥n 1: Llenar todos los huecos con 2 cocheras**\n",
    "\n",
    "Es la idea mas simple y sencilla. \n",
    "\n",
    "\n",
    "**Opci√≥n 2: Llenar los huecos siguiendo la probabilidad observada**\n",
    "\n",
    "Podemos cambiar la frecuencia de los parqueaderos y alternarlo, podemos decir que el 45% de las propiedades tienen 1 y el resto (un 55%) tienen 2. Entonces, podr√≠amos rellenar nuestros 62 valores faltantes de la siguiente manera:\n",
    "\n",
    "* Asignar el valor 1 al 45% de los huecos.\n",
    "* Asignar el valor 2 al 55% restante de los huecos.\n",
    "\n",
    "Esto intentar√≠a mantener la proporci√≥n de 1 y 2 cocheras que ya vemos en nuestros datos.\n",
    "\n",
    "Es muy importante verificar que hicimos los cambios correctos, por esto se debe volver a contar los valotes nulos en la columna car y ver cuantas veces aparecen los valores 1 y 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df242ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Primer intento: Llenar todos los huecos de 'Car' con el valor 2**\n",
    "\n",
    "# Vamos a empezar con la opci√≥n m√°s sencilla: llenar todos los valores faltantes en la columna 'Car' con el n√∫mero 2. \n",
    "# Despu√©s de hacer esto, vamos a verificar cu√°ntas veces aparece el n√∫mero 2 en la columna 'Car' y cu√°ntos valores nulos \n",
    "#quedan (esperamos que sean cero).\n",
    "\n",
    "data_car_2_mask = data.Car == 2\n",
    "data_car_2_count = data_car_2_mask.sum()\n",
    "print(data_car_2_count)\n",
    "print(\"---\")\n",
    "data_car_null_mask = data.Car.isnull()\n",
    "data_car_null_count = data_car_null_mask.sum()\n",
    "print(data_car_null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ec491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rellenamos\n",
    "\n",
    "data_car_2_fill = data.Car.fillna(2)\n",
    "# inventamos una columna nueva para no modificar los datos originales y \n",
    "# que nos sirva para el siguiente ejercicio sin volver a leer los datos:\n",
    "data[\"Car_fill\"] = data_car_2_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21976442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos nuevamente\n",
    "\n",
    "data_car_2_mask = data.Car_fill == 2\n",
    "data_car_2_count = data_car_2_mask.sum()\n",
    "print(data_car_2_count)\n",
    "print(\"---\")\n",
    "data_car_null_mask = data.Car_fill.isnull()\n",
    "data_car_null_count = data_car_null_mask.sum()\n",
    "print(data_car_null_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04252b6",
   "metadata": {},
   "source": [
    "¬°Exacto! Si antes ten√≠amos 5591 registros con alg√∫n valor en 'Car' y le sumamos los 62 que rellenamos con el valor 2, ahora deber√≠amos tener un total de 5653 registros con valores en esta columna. \n",
    "\n",
    "**Segundo intento: Llenar los huecos de 'Car' con una proporci√≥n de 1 y 2**\n",
    "\n",
    "Ahora, vamos a probar la segunda estrategia: rellenar los 62 valores faltantes en 'Car' asignando el valor 1 al 45% de ellos y el valor 2 al 55% restante. Esto se basa en la idea de que los valores 1 y 2 eran los m√°s comunes en los datos que s√≠ ten√≠amos.\n",
    "\n",
    "Para hacer esto, primero necesitamos calcular cu√°ntos de los 62 huecos deber√≠an llenarse con 1 y cu√°ntos con 2:\n",
    "\n",
    "* Cantidad de 1s: 45% de 62 = 0.45 * 62 ‚âà 28\n",
    "* Cantidad de 2s: 55% de 62 = 0.55 * 62 ‚âà 34\n",
    "\n",
    "As√≠ que, nuestro objetivo es llenar aproximadamente 28 de los huecos con el valor 1 y los 34 restantes con el valor 2.\n",
    "\n",
    "**Antes de empezar, veamos cu√°ntos registros tenemos actualmente para cada valor en la columna 'Car' (incluyendo los nulos):**\n",
    "\n",
    "Esto nos dar√° una l√≠nea base para comparar despu√©s de realizar la imputaci√≥n proporcional. Queremos ver la cantidad de valores nulos (que deber√≠a ser 0 despu√©s de la imputaci√≥n) y la cantidad de registros con 1 y 2 en 'Car'.\n",
    "\n",
    "(Aqu√≠ probablemente ir√≠a el c√≥digo para mostrar esos conteos, algo como `data['Car'].value_counts(dropna=False)`)\n",
    "\n",
    "Despu√©s de esto, implementaremos el c√≥digo para seleccionar aleatoriamente los √≠ndices de los valores nulos y asignarles los valores 1 y 2 en las proporciones calculadas. Finalmente, volveremos a verificar los conteos para confirmar que la imputaci√≥n se realiz√≥ correctamente y que las proporciones de 1 y 2 son las esperadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_car_null = data.Car.isnull().sum()\n",
    "print(cant_car_null)\n",
    "\n",
    "car_one_mask = data.Car == 1\n",
    "cant_car_1 = car_one_mask.sum()\n",
    "print(cant_car_1)\n",
    "\n",
    "car_two_mask = data.Car == 2\n",
    "cant_car_2 = car_two_mask.sum()\n",
    "print(cant_car_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3657843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# los registros que son null en Car:\n",
    "data_car_null_mask = data.Car.isnull()\n",
    "data_car_null = data.loc[data_car_null_mask, :]\n",
    "print(data_car_null.shape[0])\n",
    "\n",
    "# una muestra del 45% de los registros calculados en el paso anterior:\n",
    "data_car_null_mask_sample_1 = data_car_null.sample(frac = 0.45)\n",
    "\n",
    "# los √≠ndices de ese 45%\n",
    "data_car_null_ones_index = data_car_null_mask_sample_1.index\n",
    "print(len(data_car_null_ones_index))\n",
    "\n",
    "# los que van a ser rellenados con valor 2 son todos los que no fueron seleccionados en el paso anterior:\n",
    "data_car_null_twos_index = data_car_null.index.difference(data_car_null_ones_index)\n",
    "print(len(data_car_null_twos_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550265cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teniendo los Indes asignamos los valores \n",
    "\n",
    "data.loc[data_car_null_ones_index, \"Car\"] = 1\n",
    "data.loc[data_car_null_twos_index, \"Car\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c89fee",
   "metadata": {},
   "source": [
    "¬°Perfecto! Ya tenemos los planes para rellenar los 62 \"huecos\" en la columna 'Car': asignar el valor 1 a 28 de ellos y el valor 2 a los 34 restantes, manteniendo as√≠ una proporci√≥n similar a la que vimos en los datos originales.\n",
    "\n",
    "**Verificando los Resultados de la Imputaci√≥n Proporcional en 'Car'**\n",
    "\n",
    "Antes de realizar la imputaci√≥n, ten√≠amos los siguientes conteos en la columna 'Car':\n",
    "\n",
    "* **Valores nulos (NaN):** 62\n",
    "* **Valor 1:** 5509 registros\n",
    "* **Valor 2:** 5591 registros\n",
    "\n",
    "Despu√©s de nuestra imputaci√≥n proporcional, donde asignamos 28 valores de 1 y 34 valores de 2 a los antiguos huecos, deber√≠amos tener los siguientes conteos:\n",
    "\n",
    "* **Valores nulos (NaN):** 0 (¬°Esperamos haber llenado todos los huecos!)\n",
    "* **Valor 1:** 5509 (originales) + 28 (imputados) = **5537 registros**\n",
    "* **Valor 2:** 5591 (originales) + 34 (imputados) = **5625 registros**\n",
    "\n",
    "Ahora, el siguiente paso es **ejecutar el c√≥digo en Python para contar los valores en la columna 'Car' despu√©s de la imputaci√≥n** y verificar si los resultados coinciden con estos n√∫meros esperados. Esto nos confirmar√° que hemos realizado la imputaci√≥n correctamente y que ya no quedan valores nulos en esta columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5beb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_car_null = data.Car.isnull().sum()\n",
    "print(cant_car_null)\n",
    "\n",
    "car_one_mask = data.Car == 1\n",
    "cant_car_1 = car_one_mask.sum()\n",
    "print(cant_car_1)\n",
    "\n",
    "car_two_mask = data.Car == 2\n",
    "cant_car_2 = car_two_mask.sum()\n",
    "print(cant_car_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685c9ec",
   "metadata": {},
   "source": [
    "# Building Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d276165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anteriormente eliminamos esta columna debido a que sus datos exced√≠an el 40% de nulos; Sin embargo vamos hacer la \n",
    "#visualizaci√≥n para terminar de argumentar nuestro porqu√© a esta eliminaci√≥n\n",
    "#Relee el archivo original (cambia el nombre y la ruta seg√∫n tu caso)\n",
    "data_original = pd.read_csv(\"Data/melb_data.csv\")\n",
    "\n",
    "#Importamos la librer√≠a de matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Ahora s√≠ puedes graficar\n",
    "def distribution_plotter(data, label, bin_width=150, color_hist='#ffadad', color_kde='#2a9d8f'):\n",
    "    sns.set(rc={\"figure.figsize\": (10, 8)})\n",
    "    sns.set_style(\"white\")\n",
    "\n",
    "    # Histograma con densidad y KDE separados\n",
    "    sns.histplot(data, stat='density', kde=False, \n",
    "                 binwidth=bin_width, color=color_hist, edgecolor='black', alpha=0.5)\n",
    "\n",
    "    # Agregamos KDE manualmente\n",
    "    sns.kdeplot(data.dropna(), linewidth=3, color=color_kde)  # verde oscuro\n",
    "\n",
    "    plt.title('Distribuci√≥n de ' + label + '\\n', fontsize=18)\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel('Densidad')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc553db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plotter(data_original[\"BuildingArea\"], \"BuildingArea\", bin_width=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5957869e",
   "metadata": {},
   "source": [
    "Vemos que hay muchos outliers, ¬øqu√© forma toma la distribuci√≥n si nos quedamos s√≥lo con valores menor a 1000?\n",
    "\n",
    "Usemos para eso boolean indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_building_area_lt_1000_mask = data.BuildingArea < 1000\n",
    "data_building_area_lt_1000 = data.loc[data_building_area_lt_1000_mask, :]\n",
    "#distribution_plotter_warn(data_building_area_lt_1000.BuildingArea, \"BuildingArea lt 1000\")\n",
    "distribution_plotter(data_building_area_lt_1000.BuildingArea, \"BuildingArea lt 1000\", bin_width = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_building_area_lt_1000.BuildingArea.mean())\n",
    "print(data_building_area_lt_1000.shape)\n",
    "print(data_building_area_lt_1000.BuildingArea.median())\n",
    "print(data_building_area_lt_1000.BuildingArea.std())\n",
    "print(\"----\")\n",
    "print(data.BuildingArea.mean())\n",
    "print(data.shape)\n",
    "print(data.BuildingArea.median())\n",
    "print(data.BuildingArea.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccbd3c8",
   "metadata": {},
   "source": [
    "Debido a la gran cantidad de valores faltantes (m√°s del 40%) y la significativa variaci√≥n en los datos de la columna 'BuildingArea', rellenar estos huecos con un simple promedio (o mediana) no representar√≠a fielmente la informaci√≥n real. Para evitar introducir datos enga√±osos y distorsionar el an√°lisis, se decidi√≥ eliminar por completo la columna, siguiendo la misma l√≥gica aplicada a otras columnas con un alto porcentaje de valores nulos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f4d2a",
   "metadata": {},
   "source": [
    "# YearBuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_plotter(data.YearBuilt, \"YearBuilt\",0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c4737",
   "metadata": {},
   "source": [
    "Caso similar al anterior, tiene una gran cantidad de nulos y una inmensa dispersi√≥n de valores, entonces no se van a acompletar los datos faltantes en esa columna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af53f07",
   "metadata": {},
   "source": [
    "### Councilarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf879c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_plotter(data.CouncilArea, \"CouncilArea\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5100df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(data.CouncilArea)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4318cdbd",
   "metadata": {},
   "source": [
    "Aunque la variable CouncilArea contiene informaci√≥n valiosa sobre la ubicaci√≥n de las propiedades, su alta cardinalidad y la presencia de categor√≠as poco frecuentes motivaron una transformaci√≥n: agrupamos los valores con menos de 100 registros como \"Other\", conservando as√≠ las zonas m√°s representativas para facilitar el an√°lisis y reducir ruido en modelos predictivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46efa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ara facilitar la visualizaci√≥n y preparar los datos para futuros modelos, agrupamos los distritos con menos de \n",
    "##### 100 observaciones en una categor√≠a gen√©rica \"Other\"\n",
    "\n",
    "threshold = 100\n",
    "frequent_areas = data['CouncilArea'].value_counts()\n",
    "frequent_areas = frequent_areas[frequent_areas >= threshold].index\n",
    "\n",
    "data['CouncilArea_clean'] = data['CouncilArea'].apply(lambda x: x if x in frequent_areas else 'Other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecbb0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\": (12, 6)})\n",
    "sns.countplot(x='CouncilArea_clean', data=data, order=data['CouncilArea_clean'].value_counts().index, palette='Set2')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Distribuci√≥n de propiedades por CouncilArea (agrupado)\")\n",
    "plt.ylabel(\"Cantidad\")\n",
    "plt.xlabel(\"CouncilArea\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73974527",
   "metadata": {},
   "source": [
    "# 4 An√°lisis Exploratorio EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee475fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Estilo general para los gr√°ficos\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c92f47",
   "metadata": {},
   "source": [
    "### Vamos a realizar una distribuci√≥n general de los precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f326b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['Price'], bins=50, kde=True, color=\"#69b3a2\")\n",
    "\n",
    "# Media y mediana\n",
    "media = data['Price'].mean()\n",
    "mediana = data['Price'].median()\n",
    "\n",
    "plt.axvline(media, color='red', linestyle='--', linewidth=2, label=f'Media: {media:,.0f}')\n",
    "plt.axvline(mediana, color='blue', linestyle='--', linewidth=2, label=f'Mediana: {mediana:,.0f}')\n",
    "\n",
    "plt.title('Distribuci√≥n de precios de propiedades')\n",
    "plt.xlabel('Precio en d√≥lares australianos')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009d9ee",
   "metadata": {},
   "source": [
    "La distribuci√≥n de los precios est√° claramente sesgada a la derecha, lo que significa que hay un grupo reducido de propiedades extremadamente caras (multimillonarias) que elevan de forma importante el valor promedio.\n",
    "\n",
    "üî¥ La media (l√≠nea roja) est√° desplazada hacia la derecha por estos valores extremos.\n",
    "\n",
    "üîµ La mediana (l√≠nea azul) refleja mejor el \"precio t√≠pico\" de una propiedad, ya que no se ve afectada por los valores at√≠picos.\n",
    "\n",
    "Debido a la naturaleza asim√©trica de los precios, utilizaremos la mediana como medida de tendencia central, ya que representa de forma m√°s robusta el valor central real de las propiedades.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d57bf",
   "metadata": {},
   "source": [
    "### Ahora visualizaremos el precio vs El tipo de propiedad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26352e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Type', y='Price', data=data)\n",
    "plt.title('Precio por tipo de propiedad')\n",
    "plt.xlabel('Tipo de propiedad (h = casa, u = unidad/departamento, t = townhouse)')\n",
    "plt.ylabel('Precio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c2941",
   "metadata": {},
   "source": [
    "Las **casas (h)** son las m√°s caras y con mayor dispersi√≥n, es decir que hay mucha variabilidad en sus precios, bien sea un precio mas accesible as√≠ como tipo mansi√≥.\n",
    "Las **unidades/departamentos (u)** tienen precios m√°s bajos y estables, lo que las hacen atractivas para qui√©nes buscan un acomodo a su bolsillo y menos variabilidad.\n",
    "Las **townhouses (t)** est√°n en un pundo medio en cuanto a precio y variabilidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02436b",
   "metadata": {},
   "source": [
    "### Precio vs N√∫mero de habitaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0738c073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Rooms', y='Price', data=data,  palette='Set3')\n",
    "plt.title('Precio por n√∫mero de habitaciones')\n",
    "plt.xlabel('Habitaciones')\n",
    "plt.ylabel('Precio')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb39025",
   "metadata": {},
   "source": [
    "El n√∫mero de habitaciones es un valor que ingluye pero debe realizarse con respecto a mas variables que tambi√©n influyen en el precio, se puede observar una tendencia creciente ene l precio a medida que aumentan las habitaicones (hasta 5), despu√©s de este valor la relaci√≥n deja de ser clara y consistente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17e98e",
   "metadata": {},
   "source": [
    "### Precio vs Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cbfe79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Regionname', y='Price', data=data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Precio por regi√≥n de Melbourne')\n",
    "plt.xlabel('Regi√≥n')\n",
    "plt.ylabel('Precio')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19effc5",
   "metadata": {},
   "source": [
    "La regi√≥n es una variable determinante, no solo influye el tama√±o o tipo de vivienda sino que tambi√©n el contexto urbano.\n",
    "\n",
    "Las regiones m√°s costosas son:\n",
    "- Southern Metropolitan\n",
    "- Eastern Metropolitan\n",
    "\n",
    "Las m√°s accesibles:\n",
    "- Northern Suburbs\n",
    "- Western Victoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26742710",
   "metadata": {},
   "source": [
    "### Precio vs Distancia al centro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de1d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Distance', y='Price', data=data, alpha=0.5, color='#69b3a2')\n",
    "plt.title('Precio vs. distancia al centro de Melbourne')\n",
    "plt.xlabel('Distancia (km)')\n",
    "plt.ylabel('Precio')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94ccf2",
   "metadata": {},
   "source": [
    "Se puede observar una **correlaci√≥n negativa** entre el precio y la distancia al centro, en donde  amayor distancia al centro disminuyen los precios (aunque hay excepciones porbablemen por ser zona costera)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c08c83",
   "metadata": {},
   "source": [
    "### Mapa de precios (Lattitude vs Longtitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882cace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='Longtitude', y='Lattitude', hue='Price', data=data, palette='viridis', alpha=0.6)\n",
    "plt.title('Distribuci√≥n geogr√°fica del precio')\n",
    "plt.xlabel('Longitud')\n",
    "plt.ylabel('Latitud')\n",
    "plt.legend(title='Precio', loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c308fd9",
   "metadata": {},
   "source": [
    "Visualizando latitud y longitud coloreados por precio podemos observar que las propiedades mas cosotosas tienen a centrarse en ciertas zonas (centro o √°reas sobresalientes en el sur y sureste), los precios mas accesibles est√°n ubicados en los extremos norte y oeste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c72f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "###√Årea del edificio o terreno\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.scatterplot(data=data, x='Landsize', y='Price', ax=axs[0])\n",
    "axs[0].set_title('Precio vs tama√±o del terreno')\n",
    "axs[0].set_xlim(0, 1000)\n",
    "\n",
    "sns.scatterplot(data=data, x='BuildingArea', y='Price', ax=axs[1])\n",
    "axs[1].set_title('Precio vs √°rea del edificio')\n",
    "axs[1].set_xlim(0, 500)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d6122",
   "metadata": {},
   "source": [
    "En general, hay una tendencia clara: propiedades m√°s grandes tienden a tener precios m√°s altos. No obstante, la relaci√≥n no es totalmente lineal, y hay muchas excepciones. Algunos precios muy altos pueden deberse a ubicaci√≥n o caracter√≠sticas especiales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b711404",
   "metadata": {},
   "source": [
    "### 5. Correlaciones num√©ricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericas = data.select_dtypes(include=np.number)\n",
    "correlacion = numericas.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlacion, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Matriz de correlaci√≥n entre variables num√©ricas')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78372678",
   "metadata": {},
   "source": [
    "### Correlaci√≥n entre variables num√©ricas\n",
    "\n",
    "- El precio se relaciona positivamente con:\n",
    "    Landsize (tama√±o del terreno)\n",
    "    BuildingArea (tama√±o construido)\n",
    "    Rooms (habitaciones), aunque en menor medida.\n",
    "    \n",
    "- Correlaci√≥n negativa con la distancia al centro: mientras m√°s lejos, menor el precio.\n",
    "\n",
    "- Variables como Car y YearBuilt muestran correlaciones d√©biles, posiblemente por:\n",
    "    Datos faltantes\n",
    "    Poca influencia real en el precio\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f32e1",
   "metadata": {},
   "source": [
    "## 6. Conclusiones generales\n",
    "\n",
    "\n",
    "- El precio est√° sesgado a la derecha lo que indica que no sigue una distribuci√≥n normal por esta raz√≥n se usa la mediana como medida principal.\n",
    "- El tipo de propiedad y la regi√≥n son factores determinantes:\n",
    "    ‚Ä¢ Las casas (h) son m√°s caras y dispersas.\n",
    "    ‚Ä¢ Las regiones Southern y Eastern Metropolitan concentran precios altos.\n",
    "- A mayor distancia al centro, menor precio (con algunas excepciones).\n",
    "- Variables como Landsize, BuildingArea y Rooms tambi√©n influyen de forma importante.\n",
    "- A pesar de las correlaciones claras, existe dispersi√≥n significativa.\n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020964f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dhdsblend2021] *",
   "language": "python",
   "name": "conda-env-dhdsblend2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
