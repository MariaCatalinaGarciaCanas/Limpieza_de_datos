{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aedbc2d6",
   "metadata": {},
   "source": [
    "# Análisis Inmobiliario\n",
    "\n",
    "1. Introducción\n",
    "2. Exploración del Dataset\n",
    "3. Limpieza de datos\n",
    "4. Análisis Exploratorio de Datos (EDA)\n",
    "5. Correlaciones \n",
    "6. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4d54a",
   "metadata": {},
   "source": [
    "# 1 Introducción:\n",
    "# Insights del Mercado de Vivienda de Melbourne en 2017: Inmersión en los datos inmobiliarios\n",
    "\n",
    "![Insights Inmobiliarios](casas.jpg)\n",
    "\n",
    "Varios colegas de mi universidad han emigrado hacia Australia, en donde han encontrado oportunidades para desarrollarse como profesionales,  algunos de ellos me han contado de su interés por vivir una temporada larga allí, por lo que me entró la curiosidad. ¿Cuánto puede costar una casa en dicho país? Así que decidí escoger este data set por lo mismo, aunque es del año 2017, recopila información sobre propiedades vendidas, cada fila representa el historial o trazabilidad de una propiedad que fue vendida y cada columna nos da el detalle de dicha propiedad, precio, ubicación entre otros. \n",
    "\n",
    "La información proviene de un dataset público que enocntré en Kaggle [https://www.kaggle.com/dansbecker/melbourne-housing-snapshot](https://www.kaggle.com/dansbecker/melbourne-housing-snapshot) La información fue recopilada y organizada a partir de anuncios extraidos del sitio web:[https://www.domain.com.au/](https://www.domain.com.au/).\n",
    "\n",
    "Respecto a las variables que se encuentran en el archivo te muestro lo que significa cada columna de estos datos:\n",
    "\n",
    "* **Rooms:** El número de cuartos que tiene el lugar, que nos ayuda a entender el tamaño de la propiedad.\n",
    "* **Price:** El precio en dólares australianos al que se vendió, decisivo en el análisis del costo de las propiedades en ese momento.\n",
    "* **Method:** Una abreviatura de cómo se vendió (por ejemplo, 'S' si fue una venta normal, 'SP' si se vendió antes de la subasta). Esto nos da una idea de las estrategias de venta.\n",
    "* **Type:** Si era una casa ('h'), un apartamento ('u'), etc. Conocer el tipo nos permite comparar diferentes opciones de vivienda.\n",
    "* **SellerG:** El nombre del agente o la agencia que vendió la propiedad.\n",
    "* **Date:** El día en que se cerró la venta.\n",
    "* **Distance:** Qué tan lejos está del centro de Melbourne. Esta distancia puede influir en el precio y la conveniencia de la ubicación.\n",
    "* **Regionname:** La zona de Melbourne donde está la propiedad (por ejemplo, el oeste, el norte), que nos ayuda a analizar el mercado por áreas.\n",
    "* **Propertycount:** Cuántas propiedades hay en ese mismo barrio.\n",
    "* **Bedroom2:** El número de habitaciones según otra fuente de datos que encontré.\n",
    "* **Bathroom:** Cuántos baños tiene.\n",
    "* **Car:** Cuántos puestos para estacionar carros hay.\n",
    "* **Landsize:** El tamaño del terreno en metros cuadrados.\n",
    "* **BuildingArea:** Cuántos metros cuadrados construidos tiene la propiedad.\n",
    "* **CouncilArea:** El municipio al que pertenece.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfdfc4c",
   "metadata": {},
   "source": [
    "# 2. Exploración del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librería pandas para visualizar en modo de tabla en este notebook nuestros datos\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ruta: Creamos una variable para almacenar la ubicación del archivo CSV, que nos ayuda acortar el codigo.\n",
    "\n",
    "data_ruta = \"Data/melb_data.csv\"\n",
    "\n",
    "# Leemos el archivo CSV utilizando la función read_csv de pandas y lo cargamos en un DataFrame llamado: data.\n",
    "data = pd.read_csv(data_ruta)\n",
    "\n",
    "# Utilizamos el método head(5) para mostrar las primeras 5 filas del DataFrame 'data', lo que nos da\n",
    "# una vista rápida del contenido del dataset.\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f674f9",
   "metadata": {},
   "source": [
    "# 3. Limpieza de datos\n",
    "\n",
    "### Detección de Valores Faltantes: Buscando los \"huecos\" en la información\n",
    "\n",
    "Así como para cher es fundamental que un oufit no esté incompleto, debemos revisar nuestro DataSet, ¿Tiene valores faltantes?\n",
    "\n",
    "Asi que debemos ir a nuestro Dtaframe 'data' y revisar si hay información que falta, esto es muy importante porque si hay una columna con falta de información la confiabilidad de nuestros análisis podría disminuir drásticamente.\n",
    "\n",
    "Dado de esto podemos usar la función de pandas **'.isnull()'** ¿Para que sirve? Este método que crea una nueva tabla del mismo tamaño, pero reemplaza los datos originales por True y False.\n",
    "\n",
    "True:Cuando hay un valor faltante y se observa como Nan (not a number).\n",
    "False: Cuando hay información.\n",
    "\n",
    "Así que gracias a este método podemos tener una data con verdaderos y falsos lo que nos ayuda a identificar facilemte los \"huecos\" en nuestra información.\n",
    "\n",
    "Extraido de [documentación oficial de pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed39d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos este método y visualizamos los primeros 5 resultados\n",
    "\n",
    "is_null_result = data.isnull()\n",
    "print(is_null_result.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb9918",
   "metadata": {},
   "source": [
    "### Detección de Valores no Faltantes: Buscando los \"huecos\" en la información con .notnul()\n",
    "\n",
    "\n",
    "Tenemos también otro método, pero en este caso devuelve valor que **no son nulos** es denominado **.notnull()** \n",
    "\n",
    "\n",
    "False:Cuando hay un valor faltante y se observa como Nan (not a number).\n",
    "True: Cuando hay información.\n",
    "\n",
    "En resumen .notnull es el inverso de .isnull\n",
    "Extraído de: [documentación oficial de pandas para `.notnull()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.notnull.html) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizamos este método para visualizar los primeros resultados\n",
    "\n",
    "is_notnull_result = data.notnull()\n",
    "print(is_notnull_result.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77139b6f",
   "metadata": {},
   "source": [
    "### Conteo de los \"huecos\" por cada columna de la tabla\n",
    "\n",
    "Ahora debemos averiguar que partes de nuestra información están incompletas para tomar una decisións al respecto, recordando el método de **.isnull()** cada \"Verdadero\" (que marcaba un hueco) es como un 1 y cada \"Falso\" (donde sí había información) es como un 0.\n",
    "\n",
    "Si sumamos todos esos 1s y 0s por cada columna de nuestra tabla de \"Verdadero\" y \"Falso\" (la que obtuvimos con **.isnull()** el resultado será **el número total de \"Verdaderos\"**, lo que es igual a la cantidad de valores faltantes en esa columna.\n",
    "\n",
    "Entonces para contar cuántos valores nulos hay en cada columna de nuestro data set debemos sumar los resultados de \".isnull()\" por cada columna de la siguiente manera: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_nulls = is_null_result.sum()\n",
    "print(cant_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181fce95",
   "metadata": {},
   "source": [
    "### Conteo de Elementos No Nulos por Columna\n",
    "\n",
    "Para ver cuántos elementos no nulos tenemos en cada columna de nuestro DataFrame (`data`), podemos utilizar el método `.count()`. Este método itera a través de cada columna y devuelve el número de valores que no son considerados nulos (`NaN`).\n",
    "\n",
    "El código para realizar este conteo es el siguiente:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e2434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_null_column_count(data):\n",
    "    result = data.count()\n",
    "    return(result)\n",
    "\n",
    "resultado = not_null_column_count(data)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb8599",
   "metadata": {},
   "source": [
    "### Eliminación de Valores Faltantes (\"Drop\")\n",
    "\n",
    "Al tener identificados los valores faltantes de nuestro Dataframe debemos darles una solución, tenemos diferentes caminos, uno de ellos es la eliminación que consiste en borrar filas columnas en donde estén estos valores faltantes.\n",
    "\n",
    "Éste es un método sencillo de aplicar pero podemos tener perdida de información valiosa y posibles sesgos si hay una relación con otras variables. \n",
    "\n",
    "Implentaría éste método cuando pueda observar que la ausencia de los datos en un conjunto de datos es aleatoria, en este caso considero que debo eliminar los datos en donde los valores faltantes sean de mas del 40%; Sin embargo detallo a continuación las formas de usar pandas para realizar esta operación usando **dropna()**\n",
    "\n",
    "    \n",
    "#### Opciones de Eliminación con **dropna()** en pandas\n",
    "\n",
    "* **Eliminar columnas con *algún* valor faltante:**\n",
    "    ```python\n",
    "    data_sin_nulos_columnas = data.dropna(axis=1)\n",
    "    ```\n",
    "\n",
    "* **Eliminar filas con *todos* los valores faltantes:**\n",
    "    ```python\n",
    "    data_sin_filas_todo_nulo = data.dropna(how='all')\n",
    "    ```\n",
    "\n",
    "* **Eliminar columnas con *todos* los valores faltantes:**\n",
    "    ```python\n",
    "    data_sin_columnas_todo_nulo = data.dropna(axis=1, how='all')\n",
    "    ```\n",
    "\n",
    "* **Eliminar filas con menos de un umbral de valores no nulos:** (Ejemplo: eliminar filas con menos de 3 valores no nulos)\n",
    "    ```python\n",
    "    data_sin_filas_pocos_validos = data.dropna(thresh=3)\n",
    "    ```\n",
    "\n",
    "* **Eliminar columnas con menos de un umbral de valores no nulos:** (Ejemplo: eliminar columnas con menos de 5 valores no nulos)\n",
    "    ```python\n",
    "    data_sin_columnas_pocos_validos = data.dropna(axis=1, thresh=5)\n",
    "    ```\n",
    "\n",
    "Documentación de pandas: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Descartando Columnas con \"Demasiados\" Huecos (Umbral del 40%)\n",
    "\n",
    "#En lugar de borrar sin pensar cualquier columna con un solo dato faltante, queremos ser un poco más inteligentes. \n",
    "#La idea ahora es **eliminar solo aquellas columnas que estén \"demasiado vacías\"**.\n",
    "\n",
    "#Para esto, vamos a usar un **límite**, o como lo llamamos aquí, un **\"umbral\"**. \n",
    "#En este caso, nuestro umbral es del 40% (o 0.4). Esto significa que **si una columna tiene el 40% o más \n",
    "#de sus datos como \"huecos\" (nulos), entonces decidiremos que esa columna no nos aporta mucha información útil y la vamos a eliminar.**\n",
    "\n",
    "#Así, conservaremos las columnas que, aunque tengan algunos datos faltantes, todavía tienen una buena cantidad de información completa.\n",
    "\n",
    "#Para hacer esto, vamos a crear una función llamada `drop_columns_umbral`. Esta función va a recibir dos cosas:\n",
    "\n",
    "#* `data`: Que es nuestra tabla de datos (el DataFrame).\n",
    "#* `umbral`: Que es ese límite que definimos (en este caso, 0.4).\n",
    "\n",
    "#La función deberá revisar cada columna de nuestra tabla `data` y ver qué tan \"llena\" está. Si una columna está \"vacía\" en un 40% o más de sus filas, la función la eliminará y nos devolverá una tabla `limpia` con solo las columnas que sí tienen suficiente información.\n",
    "\n",
    "\n",
    "def drop_columns_umbral(data, umbral):\n",
    "    # Aquí vamos a escribir el código para revisar cada columna\n",
    "    # y eliminar las que tengan un porcentaje de nulos mayor o igual al 'umbral'.\n",
    "    result = None\n",
    "    return(result)\n",
    "\n",
    "umbral = 0.4\n",
    "# 'data' es la tabla de datos que ya cargamos.\n",
    "resultado = drop_columns_umbral(data, umbral)\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_umbral(data, umbral=0.4, verbose=True):\n",
    "    # 1. Calculamos el porcentaje de valores nulos por columna\n",
    "    percent_null = data.isnull().mean()\n",
    "    \n",
    "    # 2. Columnas que sí cumplen con el umbral (es decir, que tienen menos nulos del permitido)\n",
    "    columnas_conservadas = percent_null[percent_null < umbral].index\n",
    "    \n",
    "    # 3. Columnas que vamos a eliminar\n",
    "    columnas_eliminadas = percent_null[percent_null >= umbral].index\n",
    "    \n",
    "    # 4. Mostrar las columnas eliminadas si verbose está en True\n",
    "    if verbose:\n",
    "        print(f\"Columnas eliminadas (más del {umbral*100}% nulos):\")\n",
    "        print(columnas_eliminadas.tolist())\n",
    "\n",
    "    # 5. Crear dos DataFrames: uno limpio y otro solo con las columnas eliminadas\n",
    "    data_limpia = data[columnas_conservadas].copy()\n",
    "    data_columnas_eliminadas = data[columnas_eliminadas].copy()\n",
    "\n",
    "    return data_limpia, data_columnas_eliminadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_limpia, columnas_eliminadas = drop_columns_umbral(data, umbral=0.4, verbose=True)\n",
    "\n",
    "# Si más adelante quieres visualizarlas:\n",
    "columnas_eliminadas.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b83d1",
   "metadata": {},
   "source": [
    "### Llenando los \"Huecos\": Imputación de Valores Faltantes\n",
    "\n",
    "Es común tener información faltante en un conjunto de datos, por lo que debemos decidir que hacer con esa información si no se elimina se deben llenar con infromación esto se denomina imputación, hay diferentes formas de hacerlo cómo:\n",
    "\n",
    "\n",
    "**1. Rellenado Simple:**\n",
    "\n",
    "Se basa en estimar el valor faltante con la información que ya tenemos en la columna o fila.\n",
    "\n",
    "**2. Rellenado Múltiple:**\n",
    "\n",
    "Se basa en poner varios posibles valares en el lugar del dato faltante, se debe analizar y combinar los resultados para tener una idea mas clara de la incertumbre de estos datos faltantes.\n",
    "\n",
    "**3. Rellenar con el Promedio (Media):**\n",
    "\n",
    "Una forma de llenar este hueco cuando los valores son súmericos es hallar la media que es el promedio de datos de esa columna, es algo bastante sencillo, PERO si tenemos outliers puede ser un gran problema ya que los datos pueden variar enormemente. \n",
    "\n",
    "\n",
    "**4. Rellenar con un Dato Existente (Hot Deck):**\n",
    "\n",
    "Es seleccionar un valor al azar de otra parte de la columna, puede ser antes o después del \"hueco\" o uno con condiciones similares. \n",
    "\n",
    "**5. Rellenar usando Regresiones:**\n",
    "\n",
    "Esta ténica se basa en usar otras columnas de la tabla para intentar predecir el valor faltante, se debe construir un modelo para adivinar el dato.\n",
    "\n",
    "**Cómo pandas nos ayuda a rellenar los huecos:**\n",
    "\n",
    "La librería pandas de Python tiene herramientas muy útiles para la imputación, la principal es: **fillna()** y se puede usar de diferentes formas:\n",
    "\n",
    "* Usar un valor fijo: Podemos decidir llenar todos los huecos con un número específico (como 0) o un texto.\n",
    "\n",
    "* Usar el valor anterior o siguiente: Ideal para datos que cambian con el tiempo (series de tiempo). Podemos copiar el último valor válido hacia adelante **ffill** o el siguiente valor válido hacia atrás **bfill** para llenar los huecos.\n",
    "\n",
    "* Usar estadísticas: Podemos usar la media **mean()**, la moda **mode()** o la mediana **median()** de la columna para rellenar los valores faltantes. Por ejemplo: **data.fillna(data.mean())**\n",
    "\n",
    "En nuestros datos, vimos que las columnas con datos faltantes son 'Car', 'BuildingArea', 'YearBuilt' y 'CouncilArea'. \n",
    "\n",
    "Ahora vamos a intentar rellenar esos huecos usando algunas de estas ideas. Pero antes, vamos a echar un vistazo a los valores que ya tienen esas columnas para entender mejor qué método de relleno podría ser el más adecuado. Para esto, usaremos herramientas que ya aprendimos para ver cómo se distribuyen los datos.\n",
    "\n",
    "Después de esto, usaremos visualizaciones del tipo histogramas y distribuciones, para decidir como llenar los valores faltantes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2700a",
   "metadata": {},
   "source": [
    "### Echando un Vistazo a los \"Huecos\" que tenemos\n",
    "\n",
    "\n",
    "\n",
    "* Car: Le faltan 62 datos.\n",
    "* BuildingArea: Tenemos 6450 datos faltantes\n",
    "* YearBuilt: 5375 datos faltantes\n",
    "* CouncilArea: Le faltan 1369 datos.\n",
    "\n",
    "Vamos a mirar variable por variable para entender mejor que información contiene cada columna, para esto aplicamos estadística descriptiva y probabilidad.\n",
    "\n",
    "Es muy importante tener en cuenta si tenemos valores atípicos, outliers mediante las distribuciones a graficar como la normal y de esta manera poder elegir el mejor método de imputación para cada columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def count_plotter(data, label, labelsize=15, palette=\"pastel\"):    \n",
    "    sns.set(rc={\"figure.figsize\": (10, 8), \n",
    "                \"xtick.labelsize\": labelsize})\n",
    "    sns.set_style(\"white\")    \n",
    "    data_count = sns.countplot(x=data, palette=palette)\n",
    "    data_count.set_title('Histograma de ' + label + '\\n', fontsize=18)\n",
    "\n",
    "    \n",
    "#def distribution_plotter(data, label, bins=None):    \n",
    "def distribution_plotter(data, label, bin_width=150, color=\"#FF6F61\"):    \n",
    "    sns.set(rc={\"figure.figsize\": (10, 8)})\n",
    "    sns.set_style(\"white\")    \n",
    "    #dist = sns.distplot(data, bins= bins, hist_kws={'alpha':0.2}, kde_kws={'linewidth':5})\n",
    "    dist = sns.histplot(data, \n",
    "                        stat = 'density', kde = True, \n",
    "                        line_kws = {'linewidth':6}, \n",
    "                        binwidth = bin_width,\n",
    "                        color=color,  # color del histograma\n",
    "                        kde_kws={'linewidth': 3, 'color': '#34568B'})  # color del KDE)        \n",
    "    dist.set_title('Distribucion de ' + label + '\\n', fontsize=18)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d68f67",
   "metadata": {},
   "source": [
    "# Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observamos la forma de los valores de este campo \n",
    "count_plotter(data.Car, \"Car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d0516",
   "metadata": {},
   "source": [
    "Podemos usar también este método: **value_counts** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(data.Car)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f7323",
   "metadata": {},
   "source": [
    "### Rellenando los \"Huecos\" en la Cantidad de Cocheras ('Car')\n",
    "\n",
    "En la columna \"car\" podemos observar que la mayoría de propiedades tienen 1 o 2 parqueaderos que son los valores mas comunes o probables.\n",
    "\n",
    "Vamos a probar dos formas diferentes de rellenar los 62 valores faltantes que encontramos en esta columna:\n",
    "\n",
    "**Opción 1: Llenar todos los huecos con 2 cocheras**\n",
    "\n",
    "Es la idea mas simple y sencilla. \n",
    "\n",
    "\n",
    "**Opción 2: Llenar los huecos siguiendo la probabilidad observada**\n",
    "\n",
    "Podemos cambiar la frecuencia de los parqueaderos y alternarlo, podemos decir que el 45% de las propiedades tienen 1 y el resto (un 55%) tienen 2. Entonces, podríamos rellenar nuestros 62 valores faltantes de la siguiente manera:\n",
    "\n",
    "* Asignar el valor 1 al 45% de los huecos.\n",
    "* Asignar el valor 2 al 55% restante de los huecos.\n",
    "\n",
    "Esto intentaría mantener la proporción de 1 y 2 cocheras que ya vemos en nuestros datos.\n",
    "\n",
    "Es muy importante verificar que hicimos los cambios correctos, por esto se debe volver a contar los valotes nulos en la columna car y ver cuantas veces aparecen los valores 1 y 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df242ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Primer intento: Llenar todos los huecos de 'Car' con el valor 2**\n",
    "\n",
    "# Vamos a empezar con la opción más sencilla: llenar todos los valores faltantes en la columna 'Car' con el número 2. \n",
    "# Después de hacer esto, vamos a verificar cuántas veces aparece el número 2 en la columna 'Car' y cuántos valores nulos \n",
    "#quedan (esperamos que sean cero).\n",
    "\n",
    "data_car_2_mask = data.Car == 2\n",
    "data_car_2_count = data_car_2_mask.sum()\n",
    "print(data_car_2_count)\n",
    "print(\"---\")\n",
    "data_car_null_mask = data.Car.isnull()\n",
    "data_car_null_count = data_car_null_mask.sum()\n",
    "print(data_car_null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ec491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rellenamos\n",
    "\n",
    "data_car_2_fill = data.Car.fillna(2)\n",
    "# inventamos una columna nueva para no modificar los datos originales y \n",
    "# que nos sirva para el siguiente ejercicio sin volver a leer los datos:\n",
    "data[\"Car_fill\"] = data_car_2_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21976442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos nuevamente\n",
    "\n",
    "data_car_2_mask = data.Car_fill == 2\n",
    "data_car_2_count = data_car_2_mask.sum()\n",
    "print(data_car_2_count)\n",
    "print(\"---\")\n",
    "data_car_null_mask = data.Car_fill.isnull()\n",
    "data_car_null_count = data_car_null_mask.sum()\n",
    "print(data_car_null_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04252b6",
   "metadata": {},
   "source": [
    "¡Exacto! Si antes teníamos 5591 registros con algún valor en 'Car' y le sumamos los 62 que rellenamos con el valor 2, ahora deberíamos tener un total de 5653 registros con valores en esta columna. \n",
    "\n",
    "**Segundo intento: Llenar los huecos de 'Car' con una proporción de 1 y 2**\n",
    "\n",
    "Ahora, vamos a probar la segunda estrategia: rellenar los 62 valores faltantes en 'Car' asignando el valor 1 al 45% de ellos y el valor 2 al 55% restante. Esto se basa en la idea de que los valores 1 y 2 eran los más comunes en los datos que sí teníamos.\n",
    "\n",
    "Para hacer esto, primero necesitamos calcular cuántos de los 62 huecos deberían llenarse con 1 y cuántos con 2:\n",
    "\n",
    "* Cantidad de 1s: 45% de 62 = 0.45 * 62 ≈ 28\n",
    "* Cantidad de 2s: 55% de 62 = 0.55 * 62 ≈ 34\n",
    "\n",
    "Así que, nuestro objetivo es llenar aproximadamente 28 de los huecos con el valor 1 y los 34 restantes con el valor 2.\n",
    "\n",
    "**Antes de empezar, veamos cuántos registros tenemos actualmente para cada valor en la columna 'Car' (incluyendo los nulos):**\n",
    "\n",
    "Esto nos dará una línea base para comparar después de realizar la imputación proporcional. Queremos ver la cantidad de valores nulos (que debería ser 0 después de la imputación) y la cantidad de registros con 1 y 2 en 'Car'.\n",
    "\n",
    "(Aquí probablemente iría el código para mostrar esos conteos, algo como `data['Car'].value_counts(dropna=False)`)\n",
    "\n",
    "Después de esto, implementaremos el código para seleccionar aleatoriamente los índices de los valores nulos y asignarles los valores 1 y 2 en las proporciones calculadas. Finalmente, volveremos a verificar los conteos para confirmar que la imputación se realizó correctamente y que las proporciones de 1 y 2 son las esperadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_car_null = data.Car.isnull().sum()\n",
    "print(cant_car_null)\n",
    "\n",
    "car_one_mask = data.Car == 1\n",
    "cant_car_1 = car_one_mask.sum()\n",
    "print(cant_car_1)\n",
    "\n",
    "car_two_mask = data.Car == 2\n",
    "cant_car_2 = car_two_mask.sum()\n",
    "print(cant_car_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3657843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# los registros que son null en Car:\n",
    "data_car_null_mask = data.Car.isnull()\n",
    "data_car_null = data.loc[data_car_null_mask, :]\n",
    "print(data_car_null.shape[0])\n",
    "\n",
    "# una muestra del 45% de los registros calculados en el paso anterior:\n",
    "data_car_null_mask_sample_1 = data_car_null.sample(frac = 0.45)\n",
    "\n",
    "# los índices de ese 45%\n",
    "data_car_null_ones_index = data_car_null_mask_sample_1.index\n",
    "print(len(data_car_null_ones_index))\n",
    "\n",
    "# los que van a ser rellenados con valor 2 son todos los que no fueron seleccionados en el paso anterior:\n",
    "data_car_null_twos_index = data_car_null.index.difference(data_car_null_ones_index)\n",
    "print(len(data_car_null_twos_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550265cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teniendo los Indes asignamos los valores \n",
    "\n",
    "data.loc[data_car_null_ones_index, \"Car\"] = 1\n",
    "data.loc[data_car_null_twos_index, \"Car\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c89fee",
   "metadata": {},
   "source": [
    "¡Perfecto! Ya tenemos los planes para rellenar los 62 \"huecos\" en la columna 'Car': asignar el valor 1 a 28 de ellos y el valor 2 a los 34 restantes, manteniendo así una proporción similar a la que vimos en los datos originales.\n",
    "\n",
    "**Verificando los Resultados de la Imputación Proporcional en 'Car'**\n",
    "\n",
    "Antes de realizar la imputación, teníamos los siguientes conteos en la columna 'Car':\n",
    "\n",
    "* **Valores nulos (NaN):** 62\n",
    "* **Valor 1:** 5509 registros\n",
    "* **Valor 2:** 5591 registros\n",
    "\n",
    "Después de nuestra imputación proporcional, donde asignamos 28 valores de 1 y 34 valores de 2 a los antiguos huecos, deberíamos tener los siguientes conteos:\n",
    "\n",
    "* **Valores nulos (NaN):** 0 (¡Esperamos haber llenado todos los huecos!)\n",
    "* **Valor 1:** 5509 (originales) + 28 (imputados) = **5537 registros**\n",
    "* **Valor 2:** 5591 (originales) + 34 (imputados) = **5625 registros**\n",
    "\n",
    "Ahora, el siguiente paso es **ejecutar el código en Python para contar los valores en la columna 'Car' después de la imputación** y verificar si los resultados coinciden con estos números esperados. Esto nos confirmará que hemos realizado la imputación correctamente y que ya no quedan valores nulos en esta columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5beb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_car_null = data.Car.isnull().sum()\n",
    "print(cant_car_null)\n",
    "\n",
    "car_one_mask = data.Car == 1\n",
    "cant_car_1 = car_one_mask.sum()\n",
    "print(cant_car_1)\n",
    "\n",
    "car_two_mask = data.Car == 2\n",
    "cant_car_2 = car_two_mask.sum()\n",
    "print(cant_car_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685c9ec",
   "metadata": {},
   "source": [
    "# Building Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d276165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anteriormente eliminamos esta columna debido a que sus datos excedían el 40% de nulos; Sin embargo vamos hacer la \n",
    "#visualización para terminar de argumentar nuestro porqué a esta eliminación\n",
    "#Relee el archivo original (cambia el nombre y la ruta según tu caso)\n",
    "data_original = pd.read_csv(\"Data/melb_data.csv\")\n",
    "\n",
    "#Importamos la librería de matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Ahora sí puedes graficar\n",
    "def distribution_plotter(data, label, bin_width=150, color_hist='#ffadad', color_kde='#2a9d8f'):\n",
    "    sns.set(rc={\"figure.figsize\": (10, 8)})\n",
    "    sns.set_style(\"white\")\n",
    "\n",
    "    # Histograma con densidad y KDE separados\n",
    "    sns.histplot(data, stat='density', kde=False, \n",
    "                 binwidth=bin_width, color=color_hist, edgecolor='black', alpha=0.5)\n",
    "\n",
    "    # Agregamos KDE manualmente\n",
    "    sns.kdeplot(data.dropna(), linewidth=3, color=color_kde)  # verde oscuro\n",
    "\n",
    "    plt.title('Distribución de ' + label + '\\n', fontsize=18)\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel('Densidad')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc553db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plotter(data_original[\"BuildingArea\"], \"BuildingArea\", bin_width=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5957869e",
   "metadata": {},
   "source": [
    "Vemos que hay muchos outliers, ¿qué forma toma la distribución si nos quedamos sólo con valores menor a 1000?\n",
    "\n",
    "Usemos para eso boolean indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_building_area_lt_1000_mask = data.BuildingArea < 1000\n",
    "data_building_area_lt_1000 = data.loc[data_building_area_lt_1000_mask, :]\n",
    "#distribution_plotter_warn(data_building_area_lt_1000.BuildingArea, \"BuildingArea lt 1000\")\n",
    "distribution_plotter(data_building_area_lt_1000.BuildingArea, \"BuildingArea lt 1000\", bin_width = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_building_area_lt_1000.BuildingArea.mean())\n",
    "print(data_building_area_lt_1000.shape)\n",
    "print(data_building_area_lt_1000.BuildingArea.median())\n",
    "print(data_building_area_lt_1000.BuildingArea.std())\n",
    "print(\"----\")\n",
    "print(data.BuildingArea.mean())\n",
    "print(data.shape)\n",
    "print(data.BuildingArea.median())\n",
    "print(data.BuildingArea.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccbd3c8",
   "metadata": {},
   "source": [
    "Debido a la gran cantidad de valores faltantes (más del 40%) y la significativa variación en los datos de la columna 'BuildingArea', rellenar estos huecos con un simple promedio (o mediana) no representaría fielmente la información real. Para evitar introducir datos engañosos y distorsionar el análisis, se decidió eliminar por completo la columna, siguiendo la misma lógica aplicada a otras columnas con un alto porcentaje de valores nulos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f4d2a",
   "metadata": {},
   "source": [
    "# YearBuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_plotter(data.YearBuilt, \"YearBuilt\",0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c4737",
   "metadata": {},
   "source": [
    "Caso similar al anterior, tiene una gran cantidad de nulos y una inmensa dispersión de valores, entonces no se van a acompletar los datos faltantes en esa columna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af53f07",
   "metadata": {},
   "source": [
    "### Councilarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf879c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_plotter(data.CouncilArea, \"CouncilArea\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5100df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(data.CouncilArea)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4318cdbd",
   "metadata": {},
   "source": [
    "Aunque la variable CouncilArea contiene información valiosa sobre la ubicación de las propiedades, su alta cardinalidad y la presencia de categorías poco frecuentes motivaron una transformación: agrupamos los valores con menos de 100 registros como \"Other\", conservando así las zonas más representativas para facilitar el análisis y reducir ruido en modelos predictivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46efa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ara facilitar la visualización y preparar los datos para futuros modelos, agrupamos los distritos con menos de \n",
    "##### 100 observaciones en una categoría genérica \"Other\"\n",
    "\n",
    "threshold = 100\n",
    "frequent_areas = data['CouncilArea'].value_counts()\n",
    "frequent_areas = frequent_areas[frequent_areas >= threshold].index\n",
    "\n",
    "data['CouncilArea_clean'] = data['CouncilArea'].apply(lambda x: x if x in frequent_areas else 'Other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecbb0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\": (12, 6)})\n",
    "sns.countplot(x='CouncilArea_clean', data=data, order=data['CouncilArea_clean'].value_counts().index, palette='Set2')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Distribución de propiedades por CouncilArea (agrupado)\")\n",
    "plt.ylabel(\"Cantidad\")\n",
    "plt.xlabel(\"CouncilArea\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73974527",
   "metadata": {},
   "source": [
    "# 4 Análisis Exploratorio EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee475fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Estilo general para los gráficos\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c92f47",
   "metadata": {},
   "source": [
    "### Vamos a realizar una distribución general de los precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f326b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['Price'], bins=50, kde=True, color=\"#69b3a2\")\n",
    "\n",
    "# Media y mediana\n",
    "media = data['Price'].mean()\n",
    "mediana = data['Price'].median()\n",
    "\n",
    "plt.axvline(media, color='red', linestyle='--', linewidth=2, label=f'Media: {media:,.0f}')\n",
    "plt.axvline(mediana, color='blue', linestyle='--', linewidth=2, label=f'Mediana: {mediana:,.0f}')\n",
    "\n",
    "plt.title('Distribución de precios de propiedades')\n",
    "plt.xlabel('Precio en dólares australianos')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009d9ee",
   "metadata": {},
   "source": [
    "La distribución de los precios está claramente sesgada a la derecha, lo que significa que hay un grupo reducido de propiedades extremadamente caras (multimillonarias) que elevan de forma importante el valor promedio.\n",
    "\n",
    "🔴 La media (línea roja) está desplazada hacia la derecha por estos valores extremos.\n",
    "\n",
    "🔵 La mediana (línea azul) refleja mejor el \"precio típico\" de una propiedad, ya que no se ve afectada por los valores atípicos.\n",
    "\n",
    "Debido a la naturaleza asimétrica de los precios, utilizaremos la mediana como medida de tendencia central, ya que representa de forma más robusta el valor central real de las propiedades.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d57bf",
   "metadata": {},
   "source": [
    "### Ahora visualizaremos el precio vs El tipo de propiedad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26352e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Type', y='Price', data=data)\n",
    "plt.title('Precio por tipo de propiedad')\n",
    "plt.xlabel('Tipo de propiedad (h = casa, u = unidad/departamento, t = townhouse)')\n",
    "plt.ylabel('Precio')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c2941",
   "metadata": {},
   "source": [
    "Las **casas (h)** son las más caras y con mayor dispersión, es decir que hay mucha variabilidad en sus precios, bien sea un precio mas accesible así como tipo mansió.\n",
    "Las **unidades/departamentos (u)** tienen precios más bajos y estables, lo que las hacen atractivas para quiénes buscan un acomodo a su bolsillo y menos variabilidad.\n",
    "Las **townhouses (t)** están en un pundo medio en cuanto a precio y variabilidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02436b",
   "metadata": {},
   "source": [
    "### Precio vs Número de habitaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0738c073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Rooms', y='Price', data=data,  palette='Set3')\n",
    "plt.title('Precio por número de habitaciones')\n",
    "plt.xlabel('Habitaciones')\n",
    "plt.ylabel('Precio')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb39025",
   "metadata": {},
   "source": [
    "El número de habitaciones es un valor que ingluye pero debe realizarse con respecto a mas variables que también influyen en el precio, se puede observar una tendencia creciente ene l precio a medida que aumentan las habitaicones (hasta 5), después de este valor la relación deja de ser clara y consistente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17e98e",
   "metadata": {},
   "source": [
    "### Precio vs Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cbfe79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Regionname', y='Price', data=data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Precio por región de Melbourne')\n",
    "plt.xlabel('Región')\n",
    "plt.ylabel('Precio')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19effc5",
   "metadata": {},
   "source": [
    "La región es una variable determinante, no solo influye el tamaño o tipo de vivienda sino que también el contexto urbano.\n",
    "\n",
    "Las regiones más costosas son:\n",
    "- Southern Metropolitan\n",
    "- Eastern Metropolitan\n",
    "\n",
    "Las más accesibles:\n",
    "- Northern Suburbs\n",
    "- Western Victoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26742710",
   "metadata": {},
   "source": [
    "### Precio vs Distancia al centro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de1d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Distance', y='Price', data=data, alpha=0.5, color='#69b3a2')\n",
    "plt.title('Precio vs. distancia al centro de Melbourne')\n",
    "plt.xlabel('Distancia (km)')\n",
    "plt.ylabel('Precio')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94ccf2",
   "metadata": {},
   "source": [
    "Se puede observar una **correlación negativa** entre el precio y la distancia al centro, en donde  amayor distancia al centro disminuyen los precios (aunque hay excepciones porbablemen por ser zona costera)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c08c83",
   "metadata": {},
   "source": [
    "### Mapa de precios (Lattitude vs Longtitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882cace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='Longtitude', y='Lattitude', hue='Price', data=data, palette='viridis', alpha=0.6)\n",
    "plt.title('Distribución geográfica del precio')\n",
    "plt.xlabel('Longitud')\n",
    "plt.ylabel('Latitud')\n",
    "plt.legend(title='Precio', loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c308fd9",
   "metadata": {},
   "source": [
    "Visualizando latitud y longitud coloreados por precio podemos observar que las propiedades mas cosotosas tienen a centrarse en ciertas zonas (centro o áreas sobresalientes en el sur y sureste), los precios mas accesibles están ubicados en los extremos norte y oeste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c72f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Área del edificio o terreno\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.scatterplot(data=data, x='Landsize', y='Price', ax=axs[0])\n",
    "axs[0].set_title('Precio vs tamaño del terreno')\n",
    "axs[0].set_xlim(0, 1000)\n",
    "\n",
    "sns.scatterplot(data=data, x='BuildingArea', y='Price', ax=axs[1])\n",
    "axs[1].set_title('Precio vs área del edificio')\n",
    "axs[1].set_xlim(0, 500)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d6122",
   "metadata": {},
   "source": [
    "En general, hay una tendencia clara: propiedades más grandes tienden a tener precios más altos. No obstante, la relación no es totalmente lineal, y hay muchas excepciones. Algunos precios muy altos pueden deberse a ubicación o características especiales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b711404",
   "metadata": {},
   "source": [
    "### 5. Correlaciones numéricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericas = data.select_dtypes(include=np.number)\n",
    "correlacion = numericas.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlacion, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Matriz de correlación entre variables numéricas')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78372678",
   "metadata": {},
   "source": [
    "### Correlación entre variables numéricas\n",
    "\n",
    "- El precio se relaciona positivamente con:\n",
    "    Landsize (tamaño del terreno)\n",
    "    BuildingArea (tamaño construido)\n",
    "    Rooms (habitaciones), aunque en menor medida.\n",
    "    \n",
    "- Correlación negativa con la distancia al centro: mientras más lejos, menor el precio.\n",
    "\n",
    "- Variables como Car y YearBuilt muestran correlaciones débiles, posiblemente por:\n",
    "    Datos faltantes\n",
    "    Poca influencia real en el precio\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f32e1",
   "metadata": {},
   "source": [
    "## 6. Conclusiones generales\n",
    "\n",
    "\n",
    "- El precio está sesgado a la derecha lo que indica que no sigue una distribución normal por esta razón se usa la mediana como medida principal.\n",
    "- El tipo de propiedad y la región son factores determinantes:\n",
    "    • Las casas (h) son más caras y dispersas.\n",
    "    • Las regiones Southern y Eastern Metropolitan concentran precios altos.\n",
    "- A mayor distancia al centro, menor precio (con algunas excepciones).\n",
    "- Variables como Landsize, BuildingArea y Rooms también influyen de forma importante.\n",
    "- A pesar de las correlaciones claras, existe dispersión significativa.\n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020964f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dhdsblend2021] *",
   "language": "python",
   "name": "conda-env-dhdsblend2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
